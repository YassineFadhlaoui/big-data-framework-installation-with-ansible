---
# tasks file for hadoop
 - name:               detecting Home directory
   shell:              echo $HOME
   register:           home_dir


 - name:             "copying {{hadoop_package}} to remote host"
   copy:
    src:             "{{hadoop_source}}"
    dest:            "{{hadoop_destination}}"
    mode:            0644

 - name:             "decompressing {{hadoop_package}}"
   unarchive:
      src:           "{{home_dir.stdout}}/{{path_to_Hadoop}}/{{hadoop_package}}"
      dest:          "{{framework_base}}"
      copy:          no
      owner:         "{{hadoop_owner}}"
      group:         "{{hadoop_group}}"
   become:           yes

 - name: "verifying old installation"
   stat:
        path: "{{framework_base}}/hadoop"
   register: old_installation

 - name: "purge old hadoop installation"
   file:
        path: "{{framework_base}}/hadoop"
        state: absent
   when: old_installation.stat.exists == true
   become: yes

 - name:             "Renaming folder {{Uncompressed_hadoop_folder}} to hadoop"
   shell:            "mv {{framework_base}}/{{Uncompressed_hadoop_folder}} {{framework_base}}/hadoop"
   become:           yes

 - name:               testing if hadoop is already enabled
   shell:              grep HADOOP_HOME "{{home_dir.stdout}}/.bashrc"  | wc -l
   register:           isHADOOP_HOMEThere
   become:             yes

 - name:               Hadoop installation notification
   debug:              msg="{{isHADOOP_HOMEThere.stdout}}"

 - name:               "Adding Hadoop variables to .bashrc"
   lineinfile:
          path:        "{{home_dir.stdout}}/.bashrc"
          line:        "{{item}}"
   with_items:
         - "###########Hadoop installation###############"
         - "export HADOOP_HOME={{framework_base}}/hadoop"
         - "export HADOOP_CONF_DIR={{framework_base}}/hadoop/etc/hadoop"
         - "export HADOOP_MAPRED_HOME={{framework_base}}/hadoop"
         - "export HADOOP_HDFS_HOME={{framework_base}}/hadoop"
         - "export YARN_HOME={{framework_base}}/hadoop"
         - "export PATH=$PATH:{{framework_base}}/hadoop/bin:$HADOOP_HOME/sbin"
         - "############## End Hadoop variables#################"
   become:             yes
   when:               isHADOOP_HOMEThere.stdout == "0"



 - name: purge old installation
   stat:
     path: "{{home_dir.stdout}}/Hadoop/hdfs/datanode/current"
   register: old_installation

 - name: "stopping running processes"
   shell: "{{framework_base}}/hadoop/sbin/stop-all.sh"
   when: old_installation.stat.exists and old_installation.stat.isdir

 - name: "puring {{home_dir.stdout}}/Hadoop"
   file:
    state: absent
    path: "{{home_dir.stdout}}/Hadoop"
   when: old_installation.stat.exists and old_installation.stat.isdir

 - block:
    - name: "Check if datanode folder exists"
      stat:
       path: "{{item}}"
      register: hadoop_folder_stats
      with_items:
      - "{{home_dir.stdout}}/Hadoop"
      - "{{home_dir.stdout}}/Hadoop/hdfs"
      - "{{home_dir.stdout}}/Hadoop/hdfs/datanode"
      - "{{home_dir.stdout}}/Hadoop/hdfs/namenode"

    - name: "Creating folders for hadoop datanode and hadoop namenode"
      file:
       path: "{{item.item}}"
       state: directory
       mode: 0777
       owner: "{{hadoop_owner}}"
       group: "{{hadoop_group}}"
      when: item.stat.exists == false
      with_items:
      - "{{hadoop_folder_stats.results}}"

 - name: "datanode folder name"
   shell: "echo {{home_dir.stdout}}/Hadoop/hdfs/datanode"
   register: datanode_dir

 - name: "namenode folder name"
   shell: "echo {{home_dir.stdout}}/Hadoop/hdfs/namenode"
   register: namenode_dir
## configure hadoop dynamically using jinja templates
 - name: copying core-site.xml
   copy:
    src: "{{ item }}"
    dest: "{{framework_base}}/hadoop/etc/hadoop/"
    owner: "{{hadoop_owner}}"
    group: "{{hadoop_group}}"
    mode: 0644
    force: yes
   with_items:
    - "{{config_location}}/mapred-site.xml"
    - "{{config_location}}/yarn-site.xml"

   become: yes

 - name: "dynamic hdfs site configuration"
   template:
    src: "{{config_location}}/hdfs-site.j2"
    dest:  "{{framework_base}}/hadoop/etc/hadoop/hdfs-site.xml"
    owner: "{{hadoop_owner}}"
    group: "{{hadoop_group}}"
    mode: 0644


 - name: "dynamic hadoop env configuration"
   template:
    src: "{{config_location}}/hadoop-env.sh"
    dest:  "{{framework_base}}/hadoop/etc/hadoop/hadoop-env.sh"
    owner: "{{hadoop_owner}}"
    group: "{{hadoop_group}}"
    mode: 0755

 - name: "dynamic hadoop core configuration"
   template:
    src: "{{config_location}}/core-site.xml.j2"
    dest:  "{{framework_base}}/hadoop/etc/hadoop/core-site.xml"
    owner: "{{hadoop_owner}}"
    group: "{{hadoop_group}}"
    mode: 0755


 - name: "Check if id_rsa is present at {{home_dir.stdout}}/.ssh/."
   stat:
      path: "{{home_dir.stdout}}/.ssh/id_rsa"
   register: rsa_key

 - name: "Connecting to localhost without passphrase"
   shell: "ssh-keygen -t rsa -P '' -f {{home_dir.stdout}}/.ssh/id_rsa"
   when: rsa_key.stat.exists == False

 - name: "add the generated key to authorized_keys"
   shell: "cat {{home_dir.stdout}}/.ssh/id_rsa.pub >> {{home_dir.stdout}}/.ssh/authorized_keys"
   when: rsa_key.stat.exists == False

 - name: "Configuring Hadoop map reduce (1/2)"
   shell: "grep mapreduce.framework {{framework_base}}/hadoop/etc/hadoop/mapred-site.xm  | wc -l"
   register: isMapredThere
   become:  yes
 - name: "testing if java  home is set"
   shell: "cat {{framework_base}}/hadoop/etc/hadoop/hadoop-env.sh | grep -v ^# | grep JAVA_HOME | wc -l"
   register: isJAVAHomeset

 - name: adding java home
   lineinfile:
    path: "{{framework_base}}/hadoop/etc/hadoop/hadoop-env.sh"
    line: "export JAVA_HOME={{java_home}}"
    owner: "{{hadoop_owner}}"
    group: "{{hadoop_group}}"
    mode: 0755
   when: isJAVAHomeset.stdout == "0"

 - name: "stopping  node"
   shell: "{{framework_base}}/hadoop/sbin/stop-all.sh"


 - name: "formatting node"
   shell: "yes 'Y' | {{framework_base}}/hadoop/bin/hdfs namenode -format"
   register: hdfs_format

 - name: "testing the format process"
   debug: msg="namenode sucessfully formatted"
   when: hdfs_format.stdout.find("ERROR") == -1

 - name: "verifying the format process"
   debug: msg="namenode failed {{hdfs_format.stdout}}"
   when: hdfs_format.stdout.find("ERROR") != -1

 - name: "Start NameNode daemon and DataNode daemon:"
   shell: "{{framework_base}}/hadoop/sbin/start-dfs.sh"
   when: hdfs_format.stdout.find("ERROR") == -1


 - name: "Start ResourceManager daemon and NodeManager daemon"
   shell: "{{framework_base}}/hadoop/sbin/start-yarn.sh"
 - name: "Start ResourceManager daemon and NodeManager daemon"

   shell: "sleep 5"

 - name: "Verifying that Namenode and datanode are running"
   shell: "{{java_home}}/bin/jps"
   register: jps_output

 - name: "NameNode and DataNode status"
   debug: msg="Namenode and datanode successfully started"
   when: jps_output.stdout.find("NameNode") != -1  and jps_output.stdout.find("DataNode") != -1

 - name: " testing weather NameNode and DataNode were failed"
   debug: msg="Namenode and datanode failed"
   when: jps_output.stdout.find("NameNode") == -1  or jps_output.stdout.find("DataNode") == -1


 - name: "NodeManager and ResourceManager status"
   debug: msg="Namenode and datanode successfully started"
   when: jps_output.stdout.find("ResourceManager") != -1  and jps_output.stdout.find("NodeManager") != -1
